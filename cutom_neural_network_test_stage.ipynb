{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#prevent oom\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "# Normalize images to the range [0, 1]\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Flatten the images to be vectors\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(np.float32)\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(np.float32)\n",
    "\n",
    "# Convert the digit labels to even/odd labels:\n",
    "# Even -> 0, Odd -> 1\n",
    "y_train_even_odd = np.array([label % 2 for label in y_train], dtype=np.int32)\n",
    "y_test_even_odd = np.array([label % 2 for label in y_test], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayerTF:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        with tf.device(device):  # to run the operation on gpu\n",
    "            #defining the custom wegitshs and Gweights along with bias\n",
    "            self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs forward propgation of the dense neuraon\n",
    "        Params: \n",
    "        inputs : input from the previous layer or the input layer\n",
    "        Returns:\n",
    "        output : output of the dense\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "    def update_weights(self, gradients, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Updates the weights of the dense layer\n",
    "        Params: \n",
    "        gradients : gradients of the weights and bias\n",
    "        learning_rate : learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        self.weights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.Gweights.assign_sub(learning_rate * gradients[1])  # Update weights\n",
    "        self.bias.assign_sub(learning_rate * gradients[2])      # Update bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the activation functions\n",
    "class ActivationSigmoidTF:\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        perform forwrad pass for the sigmoid activation function\n",
    "        Params:\n",
    "        inputs : input from the previous layer\n",
    "        Returns:    \n",
    "        output : output of the sigmoid activation function\n",
    "        \"\"\"\n",
    "        self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "class LossBinaryCrossentropyTF:\n",
    "    def calculate(self, output, y_true):\n",
    "        \"\"\"\n",
    "        Caclulates the binary crossentropy loss\n",
    "        Params:\n",
    "        output : output of the model    \n",
    "        y_true : true labels\n",
    "        Returns:\n",
    "        loss : binary crossentropy loss\n",
    "        \"\"\"\n",
    "        output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "        return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function \n",
    "@tf.function  # Compiles function for efficiency (Graph Mode)\n",
    "def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass of the custom dense network\n",
    "        dense_layer.forward(X_batch)\n",
    "        # incorporating the activation function \n",
    "        activation.forward(dense_layer.output)\n",
    "        #calculating the loss values\n",
    "        loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.Gweights,  dense_layer.bias])\n",
    "\n",
    "    # Ensure valid gradients\n",
    "    if gradients is None or any(g is None for g in gradients):\n",
    "        return None  # If gradient calculation fails, return None\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias]))\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the main training loop:\n",
    "def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Convert input data & labels to TensorFlow tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "    # Create TensorFlow dataset for efficient training\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "    dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Initialize custom model\n",
    "    with tf.device(device):\n",
    "        dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "        activation = ActivationSigmoidTF()\n",
    "        loss_function = LossBinaryCrossentropyTF()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Adam optimizer\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "        # Process data in batches using TensorFlow dataset\n",
    "        for X_batch, y_batch in dataset:\n",
    "            result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer)\n",
    "            \n",
    "            if result is None:\n",
    "                continue  # Skip this batch if train_step() returned None\n",
    "            \n",
    "            loss, accuracy = result\n",
    "            total_loss.assign_add(loss)\n",
    "            total_accuracy.assign_add(accuracy)\n",
    "            num_batches.assign_add(1)\n",
    "\n",
    "        # Compute average loss and accuracy per epoch\n",
    "        avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "        avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "    \n",
    "    return dense_layer, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "\n",
    "# # ================== CUSTOM DENSE LAYER WITH DELTA CONDITION ==================\n",
    "# class CustomDenseLayerTF:\n",
    "#     def __init__(self, num_inputs, num_neurons):\n",
    "#         with tf.device(device):  # Ensure operations are performed on GPU if available\n",
    "#             self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "#             self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "#             self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "#             self.prev_output = None  # Store previous output for delta calculation\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         self.inputs = inputs\n",
    "#         self.output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "#     def update_weights(self, gradients, learning_rate=0.01, delta=0.5):\n",
    "#         \"\"\"\n",
    "#         Updates weights only if |zi+1 - zi| < Î´ (delta).\n",
    "#         \"\"\"\n",
    "#         if self.prev_output is not None:\n",
    "#             # Ensure prev_output matches batch size\n",
    "#             if self.prev_output.shape != self.output.shape:\n",
    "#                 self.prev_output = tf.zeros_like(self.output)  # Reset to correct shape\n",
    "\n",
    "#             output_diff = tf.abs(self.output - self.prev_output)  # Compute |zi+1 - zi|\n",
    "\n",
    "#             # Mask for elements where |zi+1 - zi| < delta\n",
    "#             update_mask = tf.cast(output_diff < delta, dtype=tf.float32)\n",
    "\n",
    "#             # Reshape `update_mask` to match weights' shape\n",
    "#             update_mask = tf.reshape(update_mask, [1, -1])  # [batch_size, num_neurons] -> [1, num_neurons]\n",
    "#             update_mask = tf.broadcast_to(update_mask, self.weights.shape)  # Expand to match weights\n",
    "\n",
    "#             # Apply updates only to neurons where the condition holds\n",
    "#             self.weights.assign_sub(learning_rate * gradients[0] * update_mask)\n",
    "#             self.Gweights.assign_sub(learning_rate * gradients[1] * update_mask)\n",
    "#             self.bias.assign_sub(learning_rate * gradients[2] * update_mask)\n",
    "\n",
    "#         # Update the previous output for the next iteration\n",
    "#         self.prev_output = tf.identity(self.output)  # Ensure shape consistency \n",
    "\n",
    "# # ================== SIGMOID ACTIVATION FUNCTION ==================\n",
    "# class ActivationSigmoidTF:\n",
    "#     def forward(self, inputs):\n",
    "#         self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "\n",
    "# # ================== BINARY CROSS-ENTROPY LOSS FUNCTION ==================\n",
    "# class LossBinaryCrossentropyTF:\n",
    "#     def calculate(self, output, y_true):\n",
    "#         output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "#         return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n",
    "\n",
    "\n",
    "# # ================== TRAINING FUNCTION WITH DELTA CONDITION ==================\n",
    "# @tf.function  # Compiles function for efficiency (Graph Mode)\n",
    "# def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # Forward pass\n",
    "#         dense_layer.forward(X_batch)\n",
    "#         activation.forward(dense_layer.output)\n",
    "#         loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "#     # Compute gradients\n",
    "#     gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias])\n",
    "\n",
    "#     # Ensure valid gradients\n",
    "#     if gradients is None or any(g is None for g in gradients):\n",
    "#         return None  # If gradient calculation fails, return None\n",
    "\n",
    "#     # Update weights based on delta condition\n",
    "#     dense_layer.update_weights(gradients, learning_rate=optimizer.learning_rate, delta=delta)\n",
    "\n",
    "#     # Compute accuracy\n",
    "#     predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "#     accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "#     return loss, accuracy\n",
    "\n",
    "\n",
    "# # ================== MAIN TRAINING FUNCTION ==================\n",
    "# def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, delta=0.5):\n",
    "#     num_samples = X_train.shape[0]\n",
    "\n",
    "#     # Convert input data & labels to TensorFlow tensors\n",
    "#     X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "#     y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "#     # Create TensorFlow dataset for efficient training\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "#     dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#     # Initialize custom model\n",
    "#     with tf.device(device):\n",
    "#         dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "#         activation = ActivationSigmoidTF()\n",
    "#         loss_function = LossBinaryCrossentropyTF()\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Adam optimizer\n",
    "\n",
    "#     # Training loop\n",
    "#     for epoch in range(epochs):\n",
    "#         total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "#         total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "#         num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "#         # Process data in batches using TensorFlow dataset\n",
    "#         for X_batch, y_batch in dataset:\n",
    "#             result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta)\n",
    "            \n",
    "#             if result is None:\n",
    "#                 continue  # Skip this batch if train_step() returned None\n",
    "            \n",
    "#             loss, accuracy = result\n",
    "#             total_loss.assign_add(loss)\n",
    "#             total_accuracy.assign_add(accuracy)\n",
    "#             num_batches.assign_add(1)\n",
    "\n",
    "#         # Compute average loss and accuracy per epoch\n",
    "#         avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "#         avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "    \n",
    "#     return dense_layer, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:36:02.387449: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-02-28 11:36:02.387495: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-28 11:36:02.387506: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-28 11:36:02.387532: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-28 11:36:02.387553: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-28 11:36:02.679688: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-28 11:36:04.269472: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.3047, Accuracy: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:36:05.326053: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.2753, Accuracy: 0.8907\n",
      "Epoch 3/10 - Loss: 0.2672, Accuracy: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:36:07.338235: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.2701, Accuracy: 0.8968\n",
      "Epoch 5/10 - Loss: 0.2529, Accuracy: 0.9001\n",
      "Epoch 6/10 - Loss: 0.2673, Accuracy: 0.8987\n",
      "Epoch 7/10 - Loss: 0.2596, Accuracy: 0.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:36:11.397350: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.2666, Accuracy: 0.8961\n",
      "Epoch 9/10 - Loss: 0.2613, Accuracy: 0.8971\n",
      "Epoch 10/10 - Loss: 0.2600, Accuracy: 0.8964\n"
     ]
    }
   ],
   "source": [
    "# Train the custom neural network with a batch size of 64\n",
    "dense_layer, activation = train_custom_nn(x_train, y_train_even_odd, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with prebuild keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â<span style=\"font-weight: bold\"> Layer (type)                    </span>â<span style=\"font-weight: bold\"> Output Shape           </span>â<span style=\"font-weight: bold\">       Param # </span>â\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â           <span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âââââââââââââââââââââââââââââââââââ³âââââââââââââââââââââââââ³ââââââââââââââââ\n",
       "â\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ\n",
       "â¡âââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ©\n",
       "â dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â           \u001b[38;5;34m785\u001b[0m â\n",
       "âââââââââââââââââââââââââââââââââââ´âââââââââââââââââââââââââ´ââââââââââââââââ\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7165 - loss: 0.5611\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8619 - loss: 0.3317\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8781 - loss: 0.2954\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8916 - loss: 0.2757\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mââââââââââââââââââââ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8930 - loss: 0.2671\n",
      "\n",
      "===== Comparison =====\n",
      "TensorFlow Neural Network Accuracy: 0.8914\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# ================== TENSORFLOW MODEL (Using GPU) ==================\n",
    "\n",
    "# Define the equivalent TensorFlow model\n",
    "with tf.device(device):  # Run on GPU\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(1, activation='sigmoid', input_shape=(28 * 28,))\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Train for 5 epochs\n",
    "    model.fit(x_train, y_train_even_odd, epochs=5, verbose=1, batch_size=32)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    loss_tf, accuracy_tf = model.evaluate(x_train, y_train_even_odd, verbose=0)\n",
    "\n",
    "print(\"\\n===== Comparison =====\")\n",
    "# print(f\"Custom Neural Network Accuracy: {acc:.4f}\")\n",
    "print(f\"TensorFlow Neural Network Accuracy: {accuracy_tf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayerTF:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        with tf.device(device):\n",
    "            self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "            self.prev_output = None  # Store previous output (Zi)\n",
    "\n",
    "    def forward(self, inputs, delta=0.5):\n",
    "        self.inputs = inputs\n",
    "        new_output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "        if self.prev_output is not None:\n",
    "            output_diff = tf.abs(new_output - self.prev_output)\n",
    "            mask = tf.cast(output_diff < delta, dtype=tf.float32)\n",
    "            self.output = mask * new_output + (1 - mask) * self.prev_output\n",
    "        else:\n",
    "            self.output = new_output  # First iteration, no previous output\n",
    "\n",
    "    def update_weights(self, gradients, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Updates the weights of the dense layer\n",
    "        Params: \n",
    "        gradients : gradients of the weights and bias\n",
    "        learning_rate : learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        self.weights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.Gweights.assign_sub(learning_rate * gradients[1])  # Update weights\n",
    "        self.bias.assign_sub(learning_rate * gradients[2])      # Update bias\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the activation functions\n",
    "class ActivationSigmoidTF:\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        perform forwrad pass for the sigmoid activation function\n",
    "        Params:\n",
    "        inputs : input from the previous layer\n",
    "        Returns:    \n",
    "        output : output of the sigmoid activation function\n",
    "        \"\"\"\n",
    "        self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "class LossBinaryCrossentropyTF:\n",
    "    def calculate(self, output, y_true):\n",
    "        \"\"\"\n",
    "        Caclulates the binary crossentropy loss\n",
    "        Params:\n",
    "        output : output of the model    \n",
    "        y_true : true labels\n",
    "        Returns:\n",
    "        loss : binary crossentropy loss\n",
    "        \"\"\"\n",
    "        output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "        return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass with delta constraint\n",
    "        dense_layer.forward(X_batch, delta=delta)\n",
    "        activation.forward(dense_layer.output)\n",
    "        loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias])\n",
    "\n",
    "    # Ensure valid gradients\n",
    "    if gradients is None or any(g is None for g in gradients):\n",
    "        return None  # If gradient calculation fails, return None\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias]))\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, delta=0.5):\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Convert input data & labels to TensorFlow tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "    # Create TensorFlow dataset for efficient training\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "    dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Initialize custom model\n",
    "    with tf.device(device):\n",
    "        dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "        activation = ActivationSigmoidTF()\n",
    "        loss_function = LossBinaryCrossentropyTF()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "        # Process data in batches using TensorFlow dataset\n",
    "        for X_batch, y_batch in dataset:\n",
    "            result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta)\n",
    "\n",
    "            if result is None:\n",
    "                continue  # Skip this batch if train_step() returned None\n",
    "\n",
    "            loss, accuracy = result\n",
    "            total_loss.assign_add(loss)\n",
    "            total_accuracy.assign_add(accuracy)\n",
    "            num_batches.assign_add(1)\n",
    "\n",
    "        # Compute average loss and accuracy per epoch\n",
    "        avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "        avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "\n",
    "    return dense_layer, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:44:19.255468: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-02-28 11:44:19.255506: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-28 11:44:19.255514: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-28 11:44:19.255532: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-28 11:44:19.255548: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-28 11:44:19.566236: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-28 11:44:21.165318: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 0.3072, Accuracy: 0.8705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:44:22.189119: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Loss: 0.2736, Accuracy: 0.8919\n",
      "Epoch 3/10 - Loss: 0.2702, Accuracy: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:44:24.220999: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 - Loss: 0.2647, Accuracy: 0.8985\n",
      "Epoch 5/10 - Loss: 0.2663, Accuracy: 0.8980\n",
      "Epoch 6/10 - Loss: 0.2691, Accuracy: 0.8979\n",
      "Epoch 7/10 - Loss: 0.2654, Accuracy: 0.8981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 11:44:28.350312: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 - Loss: 0.2660, Accuracy: 0.8971\n",
      "Epoch 9/10 - Loss: 0.2604, Accuracy: 0.9018\n",
      "Epoch 10/10 - Loss: 0.2657, Accuracy: 0.8961\n"
     ]
    }
   ],
   "source": [
    "# Train the custom neural network with a batch size of 64\n",
    "dense_layer, activation = train_custom_nn(x_train, y_train_even_odd, epochs=10, batch_size=32, delta=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
