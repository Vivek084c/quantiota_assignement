{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#prevent oom\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "# Normalize images to the range [0, 1]\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Flatten the images to be vectors\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(np.float32)\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(np.float32)\n",
    "\n",
    "# Convert the digit labels to even/odd labels:\n",
    "# Even -> 0, Odd -> 1\n",
    "y_train_even_odd = np.array([label % 2 for label in y_train], dtype=np.int32)\n",
    "y_test_even_odd = np.array([label % 2 for label in y_test], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## building the custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayerTF:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        with tf.device(device):\n",
    "            self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "            self.prev_output = None  # Store previous output (Zi)\n",
    "\n",
    "    def forward(self, inputs, delta=0.5):\n",
    "        self.inputs = inputs\n",
    "        new_output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "        if self.prev_output is not None:\n",
    "            output_diff = tf.abs(new_output - self.prev_output)\n",
    "            mask = tf.cast(output_diff < delta, dtype=tf.float32)\n",
    "            self.output = mask * new_output + (1 - mask) * self.prev_output\n",
    "        else:\n",
    "            self.output = new_output  # First iteration, no previous output\n",
    "\n",
    "    def update_weights(self, gradients, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Updates the weights of the dense layer\n",
    "        Params: \n",
    "        gradients : gradients of the weights and bias\n",
    "        learning_rate : learning rate for the optimizer\n",
    "        \"\"\"\n",
    "        self.weights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.Gweights.assign_sub(learning_rate * gradients[1])  # Update weights\n",
    "        self.bias.assign_sub(learning_rate * gradients[2])      # Update bias\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the activation functions\n",
    "class ActivationSigmoidTF:\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        perform forwrad pass for the sigmoid activation function\n",
    "        Params:\n",
    "        inputs : input from the previous layer\n",
    "        Returns:    \n",
    "        output : output of the sigmoid activation function\n",
    "        \"\"\"\n",
    "        self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "class LossBinaryCrossentropyTF:\n",
    "    def calculate(self, output, y_true):\n",
    "        \"\"\"\n",
    "        Caclulates the binary crossentropy loss\n",
    "        Params:\n",
    "        output : output of the model    \n",
    "        y_true : true labels\n",
    "        Returns:\n",
    "        loss : binary crossentropy loss\n",
    "        \"\"\"\n",
    "        output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "        return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass with delta constraint\n",
    "        dense_layer.forward(X_batch, delta=delta)\n",
    "        activation.forward(dense_layer.output)\n",
    "        loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias])\n",
    "\n",
    "    # Ensure valid gradients\n",
    "    if gradients is None or any(g is None for g in gradients):\n",
    "        return None  # If gradient calculation fails, return None\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [dense_layer.weights, dense_layer.Gweights, dense_layer.bias]))\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01, delta=0.5):\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Convert input data & labels to TensorFlow tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "    # Create TensorFlow dataset for efficient training\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "    dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Initialize custom model\n",
    "    with tf.device(device):\n",
    "        dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "        activation = ActivationSigmoidTF()\n",
    "        loss_function = LossBinaryCrossentropyTF()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "        # Process data in batches using TensorFlow dataset\n",
    "        for X_batch, y_batch in dataset:\n",
    "            result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer, delta)\n",
    "\n",
    "            if result is None:\n",
    "                continue  # Skip this batch if train_step() returned None\n",
    "\n",
    "            loss, accuracy = result\n",
    "            total_loss.assign_add(loss)\n",
    "            total_accuracy.assign_add(accuracy)\n",
    "            num_batches.assign_add(1)\n",
    "\n",
    "        # Compute average loss and accuracy per epoch\n",
    "        avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "        avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "\n",
    "    return dense_layer, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the custom neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 21:15:19.394987: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-02-28 21:15:19.395011: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-02-28 21:15:19.395017: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2025-02-28 21:15:19.395030: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-02-28 21:15:19.395039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-02-28 21:15:19.681424: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2025-02-28 21:15:21.478686: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Loss: 0.3026, Accuracy: 0.8737\n",
      "Epoch 2/2 - Loss: 0.2766, Accuracy: 0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 21:15:22.627554: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Train the custom neural network with a batch size of 64\n",
    "dense_layer, activation = train_custom_nn(x_train, y_train_even_odd, epochs=2, batch_size=32, delta=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training the prebuild tensorflow network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m785\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7657 - loss: 0.5050\n",
      "Epoch 2/2\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8626 - loss: 0.3257\n",
      "TensorFlow Neural Network Accuracy: 0.8765\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# ================== TENSORFLOW MODEL (Using GPU) ==================\n",
    "\n",
    "# Define the equivalent TensorFlow model\n",
    "with tf.device(device):  # Run on GPU\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(1, activation='sigmoid', input_shape=(28 * 28,))\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Train for 5 epochs\n",
    "    model.fit(x_train, y_train_even_odd, epochs=2, verbose=1, batch_size=32)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    loss_tf, accuracy_tf = model.evaluate(x_train, y_train_even_odd, verbose=0)\n",
    "\n",
    "\n",
    "# print(f\"Custom Neural Network Accuracy: {acc:.4f}\")\n",
    "print(f\"TensorFlow Neural Network Accuracy: {accuracy_tf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusion\n",
    "The custom neural network has an accuracy of 89.03 % with batch size = 32, epochs = 2 \n",
    "\n",
    "The prebuild neural network has an accuracy of 86.26 % with bathc size = 32, epochs =\n",
    "2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
